{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Why the SVD Rules"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Tonight, I'm going to talk about matrices and some tools for analyzing them, including the Singular Value Decomposition."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Before I get started, though, we need to have a serious talk about what it means to compute the mean of a list of numbers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "You probably think it's obvious what the mean is: if you have the numbers $(1, 2, 10)$, then their mean is just:\n",
      "\n",
      "$$\n",
      "\\frac{1 + 2 + 10}{3} = \\frac{13}{3} = 4 + \\frac{1}{3}\n",
      "$$\n",
      "\n",
      "While this is the correct technical definition, it doesn't give you any insight into why the mean is special number."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "So let's motivate the mean. The mean of an $n$-element vector, $v$, is a scalar, $s$, such that the approximation error, $E(s)$, defined as,\n",
      "\n",
      "$$\n",
      "E(s) = \\sum_{1}^{n} (v_i - s)^2,\n",
      "$$\n",
      "\n",
      "is made as small as possible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If we think this definition of approximation error, which involves squaring every error and summing them up, is a good one, then the mean is the best possible summary of $v$. This property is what's so special about the mean of a vector."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If you deeply, profoundly, understand this idea, then you can stop listening. Because the rest of my talk is just an explanation of the SVD in terms of means. As you'll see, the SVD of a matrix $M$ is the best summary of $M$ in exactly the same way that the mean, $s$, was the best summary of a vector, $v$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Before I get started, let's see how we can work with matrices in Julia."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "First, let's make a matrix that's filled with random numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = rand(10, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "10x10 Float64 Array:\n",
        " 0.696497    0.228165   0.647383   \u2026  0.751436   0.956637   0.620466\n",
        " 0.30812     0.879062   0.729225      0.115293   0.492972   0.103754\n",
        " 0.0656912   0.897911   0.0397265     0.0911406  0.728185   0.873022\n",
        " 0.561113    0.736047   0.315851      0.561876   0.618878   0.815093\n",
        " 0.853944    0.0705497  0.83472       0.0594334  0.308204   0.56559 \n",
        " 0.432       0.626918   0.579388   \u2026  0.499849   0.173665   0.790252\n",
        " 0.00696787  0.419059   0.11467       0.902229   0.0437555  0.574804\n",
        " 0.678416    0.928959   0.554722      0.502619   0.0437457  0.484711\n",
        " 0.86144     0.770327   0.368279      0.688868   0.761194   0.390276\n",
        " 0.796499    0.428956   0.670298      0.49114    0.717127   0.540856"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The `rand()` function in Julia is extremely generic. Much of its power comes from the Distributions package, which provides tools for drawing random numbers from many sophisticated probability distribitions.\n",
      "\n",
      "For example, we can create a random matrix that's filled with draws from a 2D normal distribution, which will allow us to generate a random matrix in which the two columns are strongly correlated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\u03bc = [10.0, 3.0]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "2-element Float64 Array:\n",
        " 10.0\n",
        "  3.0"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\u03a3 = [1.0 0.9;\n",
      "     0.9 1.0]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "2x2 Float64 Array:\n",
        " 1.0  0.9\n",
        " 0.9  1.0"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = MultivariateNormal(\u03bc, \u03a3)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "MultivariateNormal{PDMat} distribution\n",
        "Dim: 2\n",
        "Zeromean: false\n",
        "\u039c:\n",
        "[10.0,3.0]\n",
        "\u03a3: PDMat(2,2x2 Float64 Array:\n",
        " 1.0  0.9\n",
        " 0.9  1.0,Cholesky{Float64}(2x2 Float64 Array:\n",
        " 1.0  0.9    \n",
        " 0.9  0.43589,'U'))\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = rand(d, 1_000)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "2x1000 Float64 Array:\n",
        " 10.9426   9.81614  9.34753  9.7522   \u2026  9.31721  8.59402  9.71078  12.0739\n",
        "  4.41182  2.99471  3.2089   3.15155     1.89845  2.0004   2.36933   4.9742"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Although it may not be obvious from this short snippet, the entries of `X` are strongly related: when the first row goes up, the second row tends to follow. You can see that by using the Vega package to make a scatterplot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Vega"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = vec(X[1, :]), y = vec(X[2, :]), kind = :scatter)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": []
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This random matrix will allow us to explain why the SVD is important. As you'll see, the SVD gives us a tool for rotating this cloud of points so that they're axis aligned."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U, d, V = svd(X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(\n",
        "2x2 Float64 Array:\n",
        " -0.955526  -0.294907\n",
        " -0.294907   0.955526,\n",
        "\n",
        "[332.8130767243354,21.625703167185534],\n",
        "1000x2 Float64 Array:\n",
        " -0.0353262   0.0457124 \n",
        " -0.0308363  -0.00154075\n",
        " -0.0296807   0.0143135 \n",
        " -0.0307917   0.00626094\n",
        " -0.0301199  -0.0189177 \n",
        " -0.0347559   0.0175092 \n",
        " -0.0377446   0.0652891 \n",
        " -0.0432396   0.101943  \n",
        " -0.0346395   0.0505914 \n",
        " -0.0288943  -0.0178763 \n",
        "  \u22ee                     \n",
        " -0.0273552  -0.0365704 \n",
        " -0.0330658   0.0247797 \n",
        " -0.0303808  -0.0206503 \n",
        " -0.0375297   0.00853712\n",
        " -0.032698    0.0185486 \n",
        " -0.0353184   0.00855895\n",
        " -0.0284325  -0.0431749 \n",
        " -0.0264465  -0.0288085 \n",
        " -0.0299797  -0.0277365 \n",
        " -0.0390725   0.0551338 )"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What are the three matrices, `U`, `d` and `V` that the `svd` produced? The first answer is that they're a factorization of X: mathematically, $U * D * V = X$, where $D$ is a diagonal matrix in which the entries are the values in our vector `d`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diagm(d)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "2x2 Float64 Array:\n",
        " 332.813   0.0   \n",
        "   0.0    21.6257"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In practice, we can't expect that exact equality will hold because computers don't represent real numbers with perfect precision. Instead, we'll just check how far off the produce of `U`, `diagm(d)` and `V` is from `X`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(abs(U * diagm(d) * V' - X))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "5.729194896275658e-12"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But there's a lot more to the SVD than just this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "2x1000 Float64 Array:\n",
        " -11.757     -10.2627     -9.87814   \u2026  -8.80174   -9.97764   -13.0038 \n",
        "   0.988562   -0.0333198   0.309539     -0.623004  -0.599821    1.19231"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(x = vec(Z[1, :]), y = vec(Z[2, :]), kind = :scatter)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": []
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}